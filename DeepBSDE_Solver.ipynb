{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8u47Iy861C1"
   },
   "source": [
    "# Illustration of the DeepBSDE Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59xocsR_61C3"
   },
   "source": [
    "## General Information\n",
    "In this Jupyter notebook we illustrate the DeepBSDE solver as introduced in\n",
    "\n",
    "- W. E, J. Han and A. Jentzen. *Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations*. Communications in Mathematics and Statistics, 5, 349–380 (2017), see [published version](https://doi.org/10.1007/s40304-017-0117-6) or [arXiv preprint](https://arxiv.org/abs/1706.04702)\n",
    "- J. Han, A. Jentzen and W. E. *Solving high-dimensional partial differential equations using deep learning*. PNAS August 21, 2018 115 (34) 8505-8510, see [published version](https://doi.org/10.1073/pnas.1718942115) or [arXiv preprint](https://arxiv.org/abs/1707.02568).\n",
    "\n",
    "The code in this notebook is partially based on the [Deep BSDE GitHub repository](https://github.com/frankhan91/DeepBSDE) by Jiequn Han.\n",
    "For best results we recommend running this notebook with GPU acceleration, e.g., from within Google Colab by simply clicking the button below:\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/janblechschmidt/PDEsByNNs/blob/main/DeepBSDE_Solver.ipynb\" target=\"_parent\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ym-k4zKU61C3"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this section we extend the methodology of the [Feynman-Kac solver (GitHub)](https://github.com/janblechschmidt/PDEsByNNs/blob/main/Feynman-Kac_Solver.ipynb) to solving *semilinear* PDEs in the form of the final value problem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\partial_t u(t,x) \n",
    "   + \\frac{1}{2} \\sigma \\sigma^T(t,x) : \\nabla^2 u(t,x) \n",
    "   + \\mu(t,x) \\cdot \\nabla u(t,x) \n",
    "   + f(t,x,u(t,x),\\sigma^T(t,x) \\nabla u(t,x)) \n",
    "   &= 0, \n",
    "   \t\\quad &&(t,x) \\in [0,T) \\times \\mathbb R^d,\\\\\n",
    "   u(T,x) &= g(x), \\quad &&x \\in  \\mathbb R^d,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with drift $\\mu\\colon[0,T] \\times \\mathbb R^d \\to \\mathbb R^d$, diffusion $\\sigma\\colon[0,T] \\times\\mathbb R^d \\to \\mathbb R^{d \\times d}$ and final data $g\\colon\\mathbb R^d \\to \\mathbb R$ as before.\n",
    "The function $f\\colon [0,T] \\times \\mathbb R^d \\times \\mathbb R \\times \\mathbb R^d \\to \\mathbb R$ containing lower order terms can depend in a general way on the independent variables $t,x$ as well as on the solution $u(t,x)$  and its transformed gradient $(\\sigma^T\\nabla) u(t,x)$.\n",
    "Here, $\\nabla u(t,x)$ and $\\nabla^2 u(t,x)$ denote the gradient and Hessian of the function $u$, resp., the colon sign \"$:$\" denotes the Frobenius inner product, i.e., $A:B = \\sum_{i,j=1}^d a_{ij} \\, b_{ij}$.\n",
    "The non-divergence form of the leading-order term as well as the specific dependence on $\\sigma^T\\nabla u$ again result from the connection between PDEs and stochastic processes.\n",
    "As we will see below, the presence of these dependencies requires extending the numerical solution method to include additional approximating stochastic processes for $\\nabla u$.\n",
    "\n",
    "Semilinear PDEs of the form above arise \n",
    "in physical models such as, e.g., the Allen-Cahn, Burgers or reaction-diffusion equations; \n",
    "in finance, e.g., for pricing derivatives with default risk;\n",
    "and in stochastic control problems.\n",
    "The method discussed below is an extension to that presented in [Feynman-Kac solver (GitHub)](https://github.com/janblechschmidt/PDEsByNNs/blob/main/Feynman-Kac_Solver.ipynb) in that it is also based on the PDE-SDE connection, but in this case it is the correspondence of nonlinear PDEs with *backward stochastic differential equations (BSDEs)*.\n",
    "In the linear case discussed in [Feynman-Kac solver (GitHub)](https://github.com/janblechschmidt/PDEsByNNs/blob/main/Feynman-Kac_Solver.ipynb) the approximation of the solution $u$ at time $t=0$ is based on a neural network approximation of the mapping $u(0,\\cdot):\\mathcal D \\to \\mathbb R$, the Feynman-Kac representation $u(0,x) = \\mathbb E[g(X_T) \\, | \\, X_0 = x]$ for $x \\in \\mathcal D$ and generating a large number of \n",
    "sample paths of the stochastic process $\\{X_t\\}_{t \\in [0,T]}$ governed by an associated SDE to approximate the conditional expectation and train the model.\n",
    "Using the theory of BSDEs, it is possible to treat also nonlinearities of the type contained in the semilinear PDE.\n",
    "\n",
    "The specific method presented here was proposed in [[E, Han, Jentzen (2017)](https://doi.org/10.1007/s40304-017-0117-6)] and [[Han, Jentzen, E (2018)](https://doi.org/10.1073/pnas.1718942115).\n",
    "Again, the focus lies on solving high-dimensional problems and overcoming one source of the curse of dimensionality: a high-dimensional state space (large $d$).\n",
    "\n",
    "---\n",
    "\n",
    "The implementation below addresses the problem of evaluating the PDE solution at $t=0$ in a fixed spatial point $x$.\n",
    "However, the code can be modified to obtain the solution of the PDE at $t=0$ in a domain of interest $\\mathcal{D} \\subset \\mathbb{R}^d$, as described in [Feynman-Kac solver (GitHub)](https://github.com/janblechschmidt/PDEsByNNs/blob/main/Feynman-Kac_Solver.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VkuBL5o61C4"
   },
   "source": [
    "## Theoretical background\n",
    "The method is based on the connection of nonlinear PDEs and backward stochastic differential equations (BSDEs).\n",
    "Given a filtered probability space $(\\Omega, \\mathcal{F}, \\mathbb{P}; \\mathbb{F})$ equipped with the filtration $\\{\\mathcal{F}_t\\}_{t \\in [0,T]}$ induced by a Brownian motion $\\{W_t\\}_{t \\in [0,T]}$,\n",
    "the stochastic process $\\{X_t\\}_{t \\in [0,T]}$ describes the evolution of the state variable for all $t \\in [0,T]$ via the (forward) SDE\n",
    "\n",
    "$$\n",
    "X_t = x + \\int_0^t \\mu(s,X_s) \\, ds + \\int_0^t \\sigma(s,X_s) \\, dW_s,\n",
    "$$\n",
    "\n",
    "which directly implies $X_0 = x$.\n",
    "The backward SDE associated with this process is\n",
    "\n",
    "$$\n",
    "Y_t = g(X_T) + \\int_t^T f(s, X_s, Y_s, Z_s) \\, ds - \\int_t^T Z_s \\cdot dW_s,\n",
    "$$\n",
    "\n",
    "which in particular implies $Y_T = g(X_T)$.\n",
    "\n",
    "Under suitable regularity assumptions on the coefficients $\\mu, \\sigma, f$ and $g$, the link to the nonlinear PDE is given by the fact that for all $t \\in [0,T]$ it holds $\\mathbb{P}$-a.s. that\n",
    "\n",
    "$$\n",
    "Y_t = u(t,X_t) \\quad \\text{and} \\quad Z_t = (\\sigma^T \\nabla u) (t, X_t).\n",
    "$$\n",
    "\n",
    "Thus, the solution $u(0, x)$ can be obtained through the knowledge of $Y_0 = u(0, X_0) = u(0, x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrYwWytz61C5"
   },
   "source": [
    "## Semi-discretization in time\n",
    "We discretize the time interval $[0,T]$ into $N$ equidistant subintervals $0=t_0 < t_1 < \\ldots < t_N = T$ with constant step size $\\Delta t = T/N$.\n",
    "To approximate $\\{X_t\\}_{t \\in [0,T]}$, we employ the Euler-Maruyama scheme for the forward SDE which yields\n",
    "\n",
    "$$\n",
    "\\tilde X_{{n+1}} = \\tilde X_{n}\n",
    "+ \\mu(t_n, \\tilde X_{n}) \\, (t_{n+1} - t_n)\n",
    "+ \\sigma(t_n, \\tilde X_{n}) \\, (W_{t_{n+1}} - W_{t_n}),\n",
    "$$\n",
    "\n",
    "where $\\tilde X_n \\approx X_{t_n}$.\n",
    "Analogously, we obtain a discretization of the backward SDE as\n",
    "\n",
    "$$\n",
    "\\tilde Y_{{n+1}} = \\tilde Y_{n} \n",
    "- f(t_n, \\tilde X_{n}, \\tilde Y_{n}, \\tilde Z_{n}) \\, (t_{n+1} - t_n)\n",
    "+ \\tilde Z_{n} \\cdot ( W_{t_{n+1}} - W_{t_n}),\n",
    "$$\n",
    "\n",
    "where $\\tilde Y_n \\approx Y_{t_n}$ and $\\tilde Z_n \\approx Z_{t_n}$.\n",
    "We emphasize again that we are interested in the value $\\tilde Y_{0} \\approx Y_0 = u(0, x)$.\n",
    "Note that the increment of a Brownian motion $( W_{t_{n+1}} - W_{t_n}) \\sim \\mathsf{N}(0,(t_{n+1} - t_n) I_{d\\times d})$ is normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xha7rK7D61C5"
   },
   "source": [
    "## Implementation Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjB_pNQh61C6"
   },
   "source": [
    "### 1. Import necessary packages\n",
    "This code runs with TensorFlow versions `2.4.1`, although it should be compatible with versions `>2.3.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSd76-JJ61C6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from time import time\n",
    "\n",
    "# Set data type\n",
    "DTYPE='float32'\n",
    "#DTYPE='float64'\n",
    "tf.keras.backend.set_floatx(DTYPE)\n",
    "print('TensorFlow version used: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy6VETFV61C7"
   },
   "source": [
    "### 2. Simulation of $\\{\\tilde X_{i}\\}_{i=0, \\ldots, N}$ and $\\{W_{t_i}\\}_{i=0, \\ldots, N}$\n",
    "\n",
    "Without specifying the example now we start by setting some quantities that are important for the discretization of the processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCJRwTHW61C7"
   },
   "outputs": [],
   "source": [
    "# Final time\n",
    "T = 1.\n",
    "\n",
    "# Spatial dimensions\n",
    "dim = 100\n",
    "\n",
    "# Number of equidistant intervals in time\n",
    "N = 20\n",
    "\n",
    "# Derive time step size and t_space\n",
    "dt = T/N\n",
    "t_space = np.linspace(0, T, N + 1)\n",
    "\n",
    "# Point-of-interest at t=0\n",
    "x = np.zeros(dim)\n",
    "\n",
    "# Diffusive term is assumed to be constant\n",
    "sigma = np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-yjkPtl61C7"
   },
   "source": [
    "Next, we define a function to draw multiple sample paths of $\\{\\tilde X_{i}\\}_{i=0, \\ldots, N}$ and the corresponding increments of the Brownian motions $\\{W_{t_i}\\}_{i=0, \\ldots, N}$ according to\n",
    "\n",
    "$$\n",
    "\\tilde X_{{n+1}} = \\tilde X_{n}\n",
    "+ \\mu(t_n, \\tilde X_{n}) \\, (t_{n+1} - t_n)\n",
    "+ \\sigma(t_n, \\tilde X_{n}) \\, ( W_{t_{n+1}} - W_{t_n}), \\qquad \\tilde X_0 = x,\n",
    "$$\n",
    "\n",
    "for the particular case $\\mu(t,X_t) \\equiv 0$ and $\\sigma(t,X_t) \\equiv \\sigma I_{d \\times d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YhPPNza61C8"
   },
   "outputs": [],
   "source": [
    "def draw_X_and_dW(num_sample, x):\n",
    "    \"\"\" Function to draw num_sample many paths of the stochastic process X\n",
    "    and the corresponding increments of simulated Brownian motions dW. \"\"\"\n",
    "    \n",
    "    dim = x.shape[0]\n",
    "    \n",
    "    # Draw all increments of W at once\n",
    "    dW = np.random.normal(loc=0.0, scale=np.sqrt(dt), size=(num_sample, dim, N)).astype(DTYPE)\n",
    "\n",
    "    # Initialize the array X\n",
    "    X = np.zeros((num_sample, dim, N+1), dtype=DTYPE)\n",
    "    \n",
    "    # Set starting point to x for each draw\n",
    "    X[:, :, 0] = np.ones((num_sample, dim)) * x\n",
    "    \n",
    "    for i in range(N):\n",
    "        # This corresponds to the Euler-Maruyama Scheme\n",
    "        X[:, :, i+1] = X[:, :, i] + sigma * dW[:, :, i]\n",
    "\n",
    "    # Return simulated paths as well as increments of Brownian motion\n",
    "    return X, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpM45jTt61C9"
   },
   "source": [
    "Finally, we may test the function and draw 10 samples of the process $\\{\\tilde X_{i}\\}_{i=0,\\ldots,N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7GpOZeq61C9"
   },
   "outputs": [],
   "source": [
    "num_sample=10\n",
    "\n",
    "# Draw 10 sample paths\n",
    "X,dW = draw_X_and_dW(num_sample, np.zeros(1))\n",
    "\n",
    "# Plot these paths\n",
    "fig,ax = plt.subplots(1)\n",
    "for i in range(num_sample):\n",
    "    ax.plot(t_space,X[i,0,:])\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$X_t$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeOtvcZ461C9"
   },
   "source": [
    "### 3. Simulation of $\\{\\tilde Y_{i}\\}_{i=0, \\ldots, N}$\n",
    "A recapulation of the update formula of the value process $Y$, i.e.\n",
    "\n",
    "$$\n",
    "\\tilde Y_{{n+1}} = \\tilde Y_{n} \n",
    "- f(t_n, \\tilde X_{n}, \\tilde Y_{n}, \\tilde Z_{n}) \\, (t_{n+1} - t_n)\n",
    "+ \\tilde Z_{n} \\cdot (W_{t_{n+1}} - W_{t_n}), \\qquad \\tilde Y_N = g(\\tilde X_N),\n",
    "$$\n",
    "\n",
    "indicates that there are multiple unknowns present in this equation, namely\n",
    "\n",
    "  - $\\tilde Y_{0}$, an approximation of $u(0, x)$ as well as\n",
    "  - $\\tilde Z_{i}$, the approximations of $\\sigma^T(t_i, \\tilde X_{i}) \\nabla u(t_i, \\tilde X_{i})$ for $i = 0,\\ldots,N-1$.\n",
    "\n",
    "The unknowns $\\tilde Y_{0} \\approx u(0, x) \\in \\mathbb R$ and $\\tilde Z_{0} \\approx (\\sigma^T \\nabla u)(0, x) \\in \\mathbb R^d$ are treated as individual network parameters (we only need both of them in the particular point $(0,x)$) which are learned during training.\n",
    "\n",
    "To approximate the remaining unknowns $\\tilde Z_{i}$ we employ neural networks which realize the mappings $x \\mapsto \\sigma^T(t_i, x) \\nabla u(t_i, x)$ for $i=1,\\ldots,N$.\n",
    "\n",
    "These networks have the following structure:\n",
    "\n",
    "    Input -> BN -> (Dense -> BN -> ReLU) -> (Dense -> BN -> ReLU) -> Dense -> BN -> Output\n",
    "    \n",
    "Here, `BN` denotes Batch Normalization, `Dense` a fully connected layer **without** bias term and activation, and `ReLU` the componentwise application of the  ReLU activation function $\\mathrm{relu}(x) = \\max \\{0,x\\}$.\n",
    "All additional parameters except the initialization of `u0` and `gradu0` are chosen according to the values given in the [DeepBSDE GitHub repository](https://github.com/frankhan91/DeepBSDE) by Jiequn Han.\n",
    "\n",
    "We define the complete network as one class derived from `tf.keras.model`.\n",
    "This has several advantages, which will become clear later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkAvrqE461C-"
   },
   "outputs": [],
   "source": [
    "class BSDEModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        # Call initializer of tf.keras.Model\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Initialize the value u(0, x) randomly\n",
    "        u0 = np.random.uniform(.1, .3, size=(1)).astype(DTYPE)\n",
    "        self.u0 = tf.Variable(u0)\n",
    "        \n",
    "        # Initialize the gradient nabla u(0, x) randomly\n",
    "        gradu0 = np.random.uniform(-1e-1, 1e-1, size=(1, dim)).astype(DTYPE)\n",
    "        self.gradu0 = tf.Variable(gradu0)\n",
    "        \n",
    "        # Create template of dense layer without bias and activation\n",
    "        _dense = lambda dim: tf.keras.layers.Dense(\n",
    "            units=dim,\n",
    "            activation=None,\n",
    "            use_bias=False)\n",
    "        \n",
    "        # Create template of batch normalization layer\n",
    "        _bn = lambda : tf.keras.layers.BatchNormalization(\n",
    "            momentum=.99,\n",
    "            epsilon=1e-6,\n",
    "            beta_initializer=tf.random_normal_initializer(0.0, stddev=0.1),\n",
    "            gamma_initializer=tf.random_uniform_initializer(0.1, 0.5))\n",
    "        \n",
    "        \n",
    "        # Initialize a list of networks approximating the gradient of u(t, x) at t_i\n",
    "        self.gradui = []\n",
    "        \n",
    "        # Loop over number of time steps\n",
    "        for _ in range(N - 1):\n",
    "            \n",
    "            # Batch normalization on dim-dimensional input\n",
    "            this_grad = tf.keras.Sequential()\n",
    "            this_grad.add(tf.keras.layers.Input(dim))\n",
    "            this_grad.add(_bn())\n",
    "            \n",
    "            # Two hidden layers of type (Dense -> Batch Normalization -> ReLU)\n",
    "            for _ in range(2):\n",
    "                this_grad.add(_dense(dim+10))\n",
    "                this_grad.add(_bn())\n",
    "                this_grad.add(tf.keras.layers.ReLU())\n",
    "                \n",
    "            # Dense layer followed by batch normalization for output\n",
    "            this_grad.add(_dense(dim))\n",
    "            this_grad.add(_bn())\n",
    "            self.gradui.append(this_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyJSGc5r61C_"
   },
   "source": [
    "Next, we define a function to draw multiple realizations of $\\tilde Y_{N} \\approx u(T, X_T)$ by sweeping through the network.\n",
    "The intermediate values $\\{\\tilde Y_{i}\\}_{i=0,\\ldots,N-1}$ are not stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smbHghkx61C_"
   },
   "outputs": [],
   "source": [
    "def simulate_Y(inp, model, fun_f):\n",
    "    \"\"\" This function performs the forward sweep through the network.\n",
    "    Inputs:\n",
    "        inp - (X, dW)\n",
    "        model - model of neural network, contains\n",
    "            - u0  - variable approximating u(0, x)\n",
    "            - gradu0 - variable approximating nabla u(0, x)\n",
    "            - gradui - list of NNs approximating the mapping: x -> nabla u(t_i, x)\n",
    "        fun_f - function handle for cost function f\n",
    "    \"\"\"\n",
    "    \n",
    "    X, dW = inp\n",
    "    num_sample = X.shape[0]\n",
    "\n",
    "\n",
    "    e_num_sample = tf.ones(shape=[num_sample, 1], dtype=DTYPE)\n",
    "\n",
    "    # Value approximation at t0\n",
    "    y = e_num_sample * model.u0\n",
    "\n",
    "    # Gradient approximation at t0\n",
    "    z = e_num_sample * model.gradu0\n",
    "\n",
    "    for i in range(N-1):\n",
    "        t = t_space[i]\n",
    "        \n",
    "        # Determine terms in right-hand side of Y-update at t_i\n",
    "        eta1 = - fun_f(t, X[:, :, i], y, z) * dt\n",
    "        eta2 = tf.reduce_sum(z * dW[:, :, i], axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute new value approximations at t_{i+1}\n",
    "        y = y + eta1 + eta2\n",
    "\n",
    "        # Obtain gradient approximations at t_{i+1}\n",
    "        # Scaling the variable z by 1/dim improves the convergence properties\n",
    "        # and has been used in the original code https://github.com/frankhan91/DeepBSDE\n",
    "        # z still approximates \\sigma^T \\nabla u, but the network learns to represent\n",
    "        # a scaled version.\n",
    "        z = model.gradui[i](X[:, :, i + 1]) / dim\n",
    "\n",
    "        \n",
    "    # Final step\n",
    "    eta1 = - fun_f(t_space[N-1], X[:, :, N-1], y, z) * dt\n",
    "    eta2 = tf.reduce_sum(z * dW[:, :, N-1], axis=1, keepdims=True)\n",
    "    y = y + eta1 + eta2\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFPymaiK61DA"
   },
   "source": [
    "Through the application of this function, we may generate for each approximate state path $\\{\\tilde X_{i}\\}_{i=0,\\ldots,N}$ with corresponding Brownian motion $\\{ W_{t_i}\\}_{i=0,\\ldots,N}$ one realization of $\\tilde Y_N$ given our unknown network parameters\n",
    "\n",
    "$$\n",
    "\\theta = \\left( \\theta_{u_0}, \\theta_{\\nabla u_0}, \\theta_{\\nabla u_1}, \\ldots \\theta_{\\nabla u_{N-1}} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPN4K_AL61DA"
   },
   "source": [
    "### 4. Evaluation of loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jaL8dvM61DA"
   },
   "source": [
    "In the next step, we define the loss function, i.e., the function to be minimized.\n",
    "Since $\\tilde Y_N$ approximates $u(T, X_T) = g(X_T)$ we minimize the mean squared error (MSE) between $\\tilde Y_N$ and $g(\\tilde X_N)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFFaRVz761DA"
   },
   "outputs": [],
   "source": [
    "def loss_fn(inp, model, fun_f, fun_g):\n",
    "    \"\"\" This function computes the mean-squarred error of the difference of Y_T and g(X_T)\n",
    "    Inputs:\n",
    "        inp - (X, dW)\n",
    "        model - model of neural network containing u0, gradu0, gradui\n",
    "        fun_f - function handle for cost function f\n",
    "        fun_g - function handle for terminal condition g\n",
    "    \"\"\"\n",
    "    X, _ = inp\n",
    "    \n",
    "    # Forward pass to compute value estimates\n",
    "    y_pred = simulate_Y(inp, model, fun_f)\n",
    "        \n",
    "    # Final time condition, i.e., evaluate g(X_T)\n",
    "    y = fun_g(X[:, :, -1])\n",
    "    \n",
    "    # Compute mean squared error\n",
    "    y_diff = y-y_pred    \n",
    "    loss = tf.reduce_mean(tf.square(y_diff))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-3zoMRe61DB"
   },
   "source": [
    "### 5. Computation of the gradient w.r.t. the network parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soQK7PnW61DB"
   },
   "source": [
    "The next step uses the automatic differentiation functionality of TensorFlow to compute the gradient of the loss function with respect to the unknowns $\\theta$, called `trainable_variables` in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qorUqdF261DB"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_grad(inp, model, fun_f, fun_g):\n",
    "    \"\"\" This function computes the gradient of the loss function w.r.t.\n",
    "    the trainable variables theta.\n",
    "    Inputs:\n",
    "        inp - (X, dW)\n",
    "        model - model of neural network containing u0, gradu0, gradui\n",
    "        fun_f - function handle for cost function f\n",
    "        fun_g - function handle for terminal condition g\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(inp, model, fun_f, fun_g)\n",
    "        \n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynUw6rUZ61DB"
   },
   "source": [
    "## Solution of a linear-quadratic Gaussian control problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9KFOSoe61DB"
   },
   "source": [
    "The next code block defines the coefficient functions $f$ and $g$ for the linear-quadratic Gaussian control problem for a stochastic process\n",
    "$\\{X_t\\}_{t \\in [0,T]}$ governed by the SDE\n",
    "\n",
    "$$\n",
    "X_t = x + \\int_0^t 2 m_t \\, \\mathrm{d}t + \\int_0^t \\sqrt{2}\\, I_{d \\times d} \\, \\mathrm{d} W_t.\n",
    "$$\n",
    "\n",
    "with a control process $\\{m_t\\}_{t \\in [0,T]}$ with values in $\\mathbb{R}^d$.\n",
    "The solution of the control problem is characterized by the value function, i.e., the function $u ∶ [0, T ] \\times \\mathbb R^d \\to \\mathbb R$ that gives the minimal expected sum of accumulated running cost and final cost over all\n",
    "admissible control processes from time $t=0$ onward starting at $x$\n",
    "\n",
    "$$\n",
    "u(0,x) = \\min_{m_t} \\mathbb{E} \\left[ \\int_0^T \\|m_t\\|^2 \\mathrm{d}t + g(X_T)\\;  \\big|  \\; X_0 = x \\right],\n",
    "$$\n",
    "\n",
    "The function $g:\\mathbb{R}^d \\to \\mathbb{R}$ describes the terminal cost when the process ends at $X_T \\in \\mathbb R^d$.\n",
    "The Hamilton-Jacobi-Bellman equation associated with the stochastic\n",
    "control problem is given by the nonlinear PDE\n",
    "\n",
    "$$\n",
    "\\partial_t u(t,x) + \\Delta u(t,x) + \\min_m \\{ 2 \\, m \\cdot \\nabla u(t,x) + \\|m\\|^2\\} = 0, \\qquad u(T,x) = g(x).\n",
    "$$\n",
    "\n",
    "Note that this equation is purely deterministic.\n",
    "As one can easily see, the minimum is attained at $m = - \\nabla u$, which yields the semilinear PDE\n",
    "\n",
    "$$\n",
    "\\partial_t u(t,x) + \\Delta u(t,x) -  \\|\\nabla u(t,x)\\|^2 = 0,\n",
    "$$\n",
    "\n",
    "which will now be solved using deep neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "Remember, the drift term $\\mu$ has been set to zero in the `draw_X_and_dW` function and the diffusion has been set to $\\sigma = \\sqrt{2} I_{d \\times d}$, which reduces the term $1/2 \\, \\sigma \\sigma^T(t,x):\\nabla^2 u(t,x)$ to $\\Delta u(t,x)$.\n",
    "\n",
    "**Note**: The optimizer which performs the gradient descent step and in particular its step size (aka learning rate) have to be chosen carefully.\n",
    "The speed of convergence may depend strongly on the initialization of the unknown variables $\\theta$ as well as the the optimizer settings, e.g., a learning rate that is too high may yield a divergent sequence while values which are too low may result in slow convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9Vjr3Gy61DB"
   },
   "outputs": [],
   "source": [
    "# Define cost function f, remember that z approximates \\sigma^T \\nabla u\n",
    "def fun_f(t, x, y, z):\n",
    "    return - tf.reduce_sum(tf.square(z), axis=1, keepdims=True) / (sigma**2)\n",
    "\n",
    "# Set terminal value function g\n",
    "def fun_g(x):\n",
    "    return tf.math.log( (1+tf.reduce_sum(tf.square(x), axis=1, keepdims=True)) / 2)\n",
    "\n",
    "# Set learning rate\n",
    "lr = 1e-2\n",
    "# Choose optimizer for gradient descent step\n",
    "optimizer = tf.keras.optimizers.Adam(lr, epsilon=1e-8)\n",
    "\n",
    "# Initialize neural network architecture\n",
    "model = BSDEModel()\n",
    "y_star = 4.59016\n",
    "\n",
    "# Initialize list containing history of losses\n",
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WmDvZDa61DC"
   },
   "source": [
    "Next, we train the model for multiple epochs using batches of size 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhYCRPLI61DC"
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "\n",
    "num_epochs = 40000\n",
    "\n",
    "# Initialize header of output\n",
    "print('  Iter        Loss        y   L1_rel    L1_abs   |   Time  Stepsize')\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Each epoch we draw a batch of 64 random paths\n",
    "    X, dW = draw_X_and_dW(64, x)\n",
    "\n",
    "    # Compute the loss as well as the gradient\n",
    "    loss, grad = compute_grad((X, dW), model, fun_f, fun_g)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    \n",
    "    # Get current Y_0 \\approx u(0,x)\n",
    "    y = model.u0.numpy()[0]\n",
    "\n",
    "    currtime = time() - t0\n",
    "    l1abs = np.abs(y - y_star)\n",
    "    l1rel = l1abs / y_star\n",
    "    \n",
    "    hentry = (i, loss.numpy(), y, l1rel, l1abs, currtime, lr)\n",
    "    history.append(hentry)\n",
    "    if i%10 == 0:\n",
    "        print('{:5d} {:12.4f} {:8.4f} {:8.4f}  {:8.4f}   | {:6.1f}  {:6.2e}'.format(*hentry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3GCjw2I61DC"
   },
   "source": [
    "Plot training history and evolution of approximation of $u(0, x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rhDnPkh61DC"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(15,6))\n",
    "xrange = range(len(history))\n",
    "ax[0].semilogy(xrange, [e[1] for e in history],'k-')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('training loss')\n",
    "ax[1].plot(xrange, [e[2] for e in history])\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('$u(0,x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS2o1MNf61DC"
   },
   "source": [
    "## Class implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8uxU8RX61DC"
   },
   "source": [
    "The following code cell implements the Deep BSDE solver for the evaluation of the PDE solution in one point $(t_0,x)$ as one class, which is more handy for further experiments.\n",
    "The final problem-specific solver is then derived through subclassing by defining methods for the reaction term $f$ and final data $g$.\n",
    "\n",
    "**Note**: In order to construct a neural network approximation of the PDE solution on a domain of interest $\\mathcal D$ at a fixed time $t = t_0$ instead of at a single point $x \\in \\mathbb R^d$ the following modifications are necessary:\n",
    "- sample $x = X_0$ uniformly from $\\mathcal D$  as in the [Feynman-Kac solver (GitHub)](https://github.com/janblechschmidt/PDEsByNNs/blob/main/Feynman-Kac_Solver.ipynb),\n",
    "- replace the variables for $\\tilde Y_0$ and $\\tilde Z_0$ by appropriate neural networks which approximate the mappings $x \\mapsto u(t_0, x)$ and $u \\mapsto (\\sigma^T \\nabla u)(t_0, x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcap_79T61DD"
   },
   "outputs": [],
   "source": [
    "class DeepBSDE(tf.keras.Model):\n",
    "    def __init__(self, t0=0.0,\n",
    "                 t1=1.0,\n",
    "                 dim=100,\n",
    "                 time_steps=20,\n",
    "                 sigma=np.sqrt(2),\n",
    "                 learning_rate=1e-2,\n",
    "                 num_hidden_layers=2,\n",
    "                 num_neurons=200,\n",
    "                 **kwargs):\n",
    "        \"\"\"Set up basic architecture of deep BSDE NN model.\"\"\"\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        self.N = time_steps\n",
    "        self.dim = dim\n",
    "        self.sigma = sigma\n",
    "        self.x = 0.0*np.ones(self.dim)\n",
    "        self.dt = (t1 - t0)/(self.N)\n",
    "        self.sqrt_dt = np.sqrt(self.dt)\n",
    "        \n",
    "        self.t_space = np.linspace(self.t0, self.t1, self.N + 1)[:-1]\n",
    "        \n",
    "        # Set optimizer\n",
    "        self.optimizer=tf.keras.optimizers.Adam(learning_rate,\n",
    "                                                epsilon=1e-8)\n",
    "        \n",
    "        # Initialize value and gradient of u(t_0,X_{t_0}) by zeros\n",
    "        #self.u0 = tf.Variable(np.zeros((1),dtype=DTYPE))\n",
    "        #self.gradu0 = tf.Variable(np.zeros((1,self.dim),dtype=DTYPE))\n",
    "        \n",
    "        # Alternatively, initialize both randomly\n",
    "        self.u0 = tf.Variable(np.random.uniform(.3, .5, size=(1)).astype(DTYPE))\n",
    "        self.gradu0 = tf.Variable(np.random.uniform(-1e-1, 1e-1, size=(1, dim)).astype(DTYPE))\n",
    "        \n",
    "        # Create template of dense layer without bias and activation\n",
    "        _dense = lambda dim: tf.keras.layers.Dense(\n",
    "            units=dim,\n",
    "            activation=None,\n",
    "            use_bias=False)\n",
    "        \n",
    "        # Create template of batch normalization layer\n",
    "        _bn = lambda : tf.keras.layers.BatchNormalization(\n",
    "            momentum=.99,\n",
    "            epsilon=1e-6,\n",
    "            beta_initializer=tf.random_normal_initializer(0.0, stddev=0.1),\n",
    "            gamma_initializer=tf.random_uniform_initializer(0.1, 0.5))\n",
    "        \n",
    "        \n",
    "        # Initialize a list of networks approximating the gradient of u(t, x) at t_i\n",
    "        self.gradui = []\n",
    "        \n",
    "        # Loop over number of time steps\n",
    "        for _ in range(self.N - 1):\n",
    "            \n",
    "            # Batch normalization on dim-dimensional input\n",
    "            this_grad = tf.keras.Sequential()\n",
    "            this_grad.add(tf.keras.layers.Input(dim))\n",
    "            this_grad.add(_bn())\n",
    "            \n",
    "            # Hidden layers of type (Dense -> Batch Normalization -> ReLU)\n",
    "            for _ in range(num_hidden_layers):\n",
    "                this_grad.add(_dense(num_neurons))\n",
    "                this_grad.add(_bn())\n",
    "                this_grad.add(tf.keras.layers.ReLU())\n",
    "                \n",
    "            # Dense layer followed by batch normalization for output\n",
    "            this_grad.add(_dense(dim))\n",
    "            this_grad.add(_bn())\n",
    "            self.gradui.append(this_grad)\n",
    "      \n",
    "            \n",
    "    def draw_X_and_dW(self, num_sample):\n",
    "        \"\"\" Method to draw num_sample paths of X. \"\"\"\n",
    "        \n",
    "        # Draw increments of Brownian motion\n",
    "        dW = np.random.normal(loc=0.0, scale=self.sqrt_dt, size=(num_sample, self.dim, self.N)).astype(DTYPE)\n",
    "        \n",
    "        # Initialize and set array of paths\n",
    "        X = np.zeros((num_sample, self.dim, self.N+1), dtype=DTYPE)\n",
    "        \n",
    "        X[:, :, 0] = np.ones((num_sample, self.dim)) * self.x\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            # This corresponds to the Euler-Maruyama Scheme\n",
    "            X[:, :, i+1] = X[:, :, i] + self.sigma * dW[:, :, i]\n",
    "            \n",
    "        # Return simulated paths as well as increments of Brownian motion\n",
    "        return X, dW\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inp, training=False):\n",
    "        \"\"\"\n",
    "        Method to perform one forward sweep through the network\n",
    "        given inputs: inp - (X, dW)\n",
    "                      training - boolean variable indicating training\n",
    "        \"\"\"\n",
    "        X, dW = inp\n",
    "        num_sample = X.shape[0]\n",
    "        \n",
    "        \n",
    "        e_num_sample = tf.ones(shape=[num_sample, 1], dtype=DTYPE)\n",
    "        \n",
    "        # Value approximation at t0\n",
    "        y = e_num_sample * self.u0\n",
    "        \n",
    "        # Gradient approximation at t0\n",
    "        z = e_num_sample * self.gradu0\n",
    "        \n",
    "        for i in range(self.N-1):\n",
    "            \n",
    "            t = self.t_space[i]\n",
    "            \n",
    "            # Optimal control is attained by gradient\n",
    "            eta1 = - self.fun_f(t, X[:, :, i], y, z) * self.dt\n",
    "            eta2 = tf.reduce_sum(z * dW[:, :, i], axis=1, keepdims=True)\n",
    "\n",
    "            y = y + eta1 + eta2\n",
    "\n",
    "            # New gradient approximation\n",
    "            # The division by self.dim acts as a stabilizer\n",
    "            z = self.gradui[i](X[:, :, i + 1], training) / self.dim\n",
    "\n",
    "        # Final step\n",
    "        eta1 = - self.fun_f(self.t_space[self.N-1], X[:, :, self.N-1], y, z) * self.dt\n",
    "        eta2 = tf.reduce_sum(z * dW[:, :, self.N-1], axis=1, keepdims=True)\n",
    "\n",
    "        y = y + eta1 + eta2\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def loss_fn(self, inputs, training=False):\n",
    "        X, _ = inputs\n",
    "        # Forward pass to compute value estimates\n",
    "        y_pred = self.call(inputs, training)\n",
    "        \n",
    "        # Exact values at final time\n",
    "        y = self.fun_g(X[:, :, -1])\n",
    "        y_diff = y-y_pred\n",
    "        loss = tf.reduce_mean(tf.square(y_diff))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, inp):\n",
    "        loss, grad = self.grad(inp, training=True)\n",
    "        self.optimizer.apply_gradients(zip(grad, self.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def grad(self, inputs, training=False):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn(inputs, training)\n",
    "        grad = tape.gradient(loss, self.trainable_variables)\n",
    "        return loss, grad\n",
    "    \n",
    "    def fun_f(self, t, x, y, z):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def fun_g(self, t, x, y, z):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qG_WWYm61DD"
   },
   "source": [
    "### Example 1: Solution of the linear-quadratic Gaussian control problem\n",
    "Next, we derive a class to solve the linear-quadratic Gaussian control problem (HJB equation)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u_t(t,x) + \\Delta u(t,x) - \\|\\nabla u(t,x)\\|^2 &= 0,\\\\\n",
    "u(T,x) &= g(x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The class includes also a method `estimate_solution()` to approximate the exact solution by means of a Monte-Carlo estimation.\n",
    "In can be shown (see [E, Han, Jenzten (2017), Lemma 4.2](https://arxiv.org/abs/1706.04702) ) that the solution of the PDE in a point $(0,x)$ is given by the expression\n",
    "\n",
    "$$\n",
    "u(0,x) = \\log \\left(\n",
    "\\mathbb{E} \\left[\n",
    "\\exp(-g(x + \\sigma W_T))\n",
    "\\right]\n",
    "\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnOSxeKr61DD"
   },
   "outputs": [],
   "source": [
    "class HJBSolver(DeepBSDE):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # 'Exact' solution at (0,x) has been determined with the method estimate_solution()\n",
    "        self.y_star = 4.590147312917355\n",
    "        \n",
    "    # Running cost\n",
    "    @tf.function\n",
    "    def fun_f(self, t, x, y, z):\n",
    "        return - tf.reduce_sum(tf.square(z), axis=1, keepdims=True) / (self.sigma**2)\n",
    "\n",
    "    # Terminal/Final cost\n",
    "    @tf.function\n",
    "    def fun_g(self, x):\n",
    "        return tf.math.log( (1+tf.reduce_sum(tf.square(x), axis=1, keepdims=True)) / 2)\n",
    "    \n",
    "    def estimate_solution(self, mc_iter=1e5, N_runs=200, replace=False):\n",
    "        \"\"\"This method can be used to estimate the solution at (0,x) by means of\n",
    "        Monte-Carlo estimation.\"\"\"\n",
    "        N_total = 0\n",
    "        total_mean = 0\n",
    "        old_est = 0\n",
    "        np.random.seed(0)\n",
    "        for _ in range(int(N_runs)):\n",
    "            W = np.random.normal(loc=0.0, scale=np.sqrt(model.t1-model.t0), size=(int(mc_iter), model.dim))\n",
    "            X_T = model.x + model.sigma * W\n",
    "            this_mean = np.mean(np.exp(-model.fun_g(X_T)))\n",
    "    \n",
    "            total_mean = (N_total * total_mean + mc_iter * this_mean) / (N_total + mc_iter)\n",
    "            N_total+=mc_iter\n",
    "            total_est = - np.log(total_mean)\n",
    "            est_diff = np.abs(total_est-old_est)\n",
    "            print('Current estimate: ', total_est, '\\tDiff: ', est_diff)\n",
    "            old_est = total_est\n",
    "        if replace:\n",
    "            self.y_star = total_est\n",
    "        return total_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDuNK7Zcku9O"
   },
   "outputs": [],
   "source": [
    "model=HJBSolver()\n",
    "model.estimate_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15RZWQkC1tMY"
   },
   "source": [
    "Design small experiment for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6mb6kEZ1sLg"
   },
   "outputs": [],
   "source": [
    "def experiment(model, num_epochs=2000):\n",
    "    # Initialize header\n",
    "    print('  Iter        Loss        y   L1_rel    L1_abs   |   Time  Stepsize')\n",
    "    \n",
    "    # Init timer and history list\n",
    "    t0 = time()\n",
    "    history=[]\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        inp = model.draw_X_and_dW(batch_size)\n",
    "        loss = model.train(inp)\n",
    "\n",
    "        # Get current Y_0 \\approx u(0,x)\n",
    "        y = model.u0.numpy()[0]\n",
    "\n",
    "        currtime = time() - t0\n",
    "        l1abs = np.abs(y - model.y_star)\n",
    "        l1rel = l1abs / model.y_star\n",
    "\n",
    "        hentry = (i, loss.numpy(), y, l1rel, l1abs, currtime, lr)\n",
    "        history.append(hentry)\n",
    "        if i%100 == 0:\n",
    "            print('{:5d} {:12.4f} {:8.4f} {:8.4f}  {:8.4f}   | {:6.1f}  {:6.2e}'.format(*hentry))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nole_O9HYTKw"
   },
   "source": [
    "Parameter settings for experiments carried out in the paper, Section 4.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IBmp1fXSDjF"
   },
   "outputs": [],
   "source": [
    "num_exp = 5\n",
    "batch_size = 64\n",
    "lr = 1e-2\n",
    "dim = 100\n",
    "History = []\n",
    "prob_id = 4\n",
    "if prob_id == 1:\n",
    "    # Exp 1: Orig\n",
    "    num_neurons = 110\n",
    "    num_hidden_layers = 2\n",
    "    num_time_steps = 20\n",
    "    suffix = 'orig'\n",
    "elif prob_id == 2:\n",
    "    # Exp 2: 3 layers\n",
    "    num_neurons = 200\n",
    "    num_hidden_layers = 3\n",
    "    num_time_steps = 30\n",
    "    suffix = '3layers'\n",
    "elif prob_id == 3:\n",
    "    # Exp 3: 5 layers\n",
    "    num_neurons = 300\n",
    "    num_hidden_layers = 5\n",
    "    num_time_steps = 50\n",
    "    suffix = '5layers'\n",
    "elif prob_id == 4:\n",
    "    # Exp 4: simple model\n",
    "    num_neurons = 110\n",
    "    num_hidden_layers = 0\n",
    "    num_time_steps = 1\n",
    "    suffix = 'simple'\n",
    "\n",
    "for j in range(num_exp):\n",
    "    print('{:s}\\nStart of run {:d}\\n{:s}'.format(70*'-',j+1,70*'-'))\n",
    "\n",
    "    # Experiment 1: Linear-quadratic Gaussian control - HJB equation\n",
    "    model = HJBSolver(t1=1.,\n",
    "                            time_steps=num_time_steps,\n",
    "                            dim=dim,\n",
    "                            learning_rate=lr,\n",
    "                            num_neurons=num_neurons,\n",
    "                            num_hidden_layers=num_hidden_layers)\n",
    "    history = experiment(model, num_epochs=20000)\n",
    "    \n",
    "    History.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmr0z__TvDGU"
   },
   "source": [
    "Write statistics to `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c11ck73qsuww"
   },
   "outputs": [],
   "source": [
    "X=np.stack(History)\n",
    "epochs = X[0,:,0].astype(int)\n",
    "\n",
    "# Derive statistics for table\n",
    "Loss_mean = np.mean(X[:,:,1], axis=0)\n",
    "Loss_std = np.std(X[:,:,1], axis=0)\n",
    "y_mean = np.mean(X[:,:,2], axis=0)\n",
    "y_std = np.std(X[:,:,2], axis=0)\n",
    "L1rel_mean = np.mean(X[:,:,3], axis=0)\n",
    "L1rel_std = np.std(X[:,:,3], axis=0)\n",
    "L1abs_mean = np.mean(X[:,:,4], axis=0)\n",
    "L1abs_std = np.std(X[:,:,4], axis=0)\n",
    "Time_mean = np.mean(X[:,:,5], axis=0)\n",
    "Time_std = np.std(X[:,:,5], axis=0)\n",
    "\n",
    "# Write table\n",
    "SAVE_CSV = True\n",
    "if SAVE_CSV:\n",
    "    import csv\n",
    "    with open('hjb_dim_{:03d}_{:s}.csv'.format(dim, suffix),'w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['Iter', 'LossMean', 'LossStd', 'yMean', 'yStd', 'L1relMean', 'L1relStd', 'L1absMean', 'L1absStd', 'TimeMean','TimeStd'])\n",
    "        for i in epochs:\n",
    "            csv_out.writerow((i, Loss_mean[i], Loss_std[i],\n",
    "                              y_mean[i], y_std[i], L1rel_mean[i], L1rel_std[i],\n",
    "                              L1abs_mean[i], L1abs_std[i], Time_mean[i], Time_std[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98o3_sHzvIm1"
   },
   "source": [
    "Finally, we may plot some loss curves and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQlFVpB1bKuS"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(15,6))\n",
    "color = 'tab:blue'\n",
    "ax[0].semilogy(epochs, Loss_mean, color=color)\n",
    "ax[0].fill_between(epochs, Loss_mean-Loss_std, Loss_mean+Loss_std, alpha=0.2)\n",
    "ax[0].set_xlabel('$n_{epoch}$')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Mean loss')\n",
    "\n",
    "ax[1].plot(epochs, y_mean, color=color)\n",
    "ax[1].fill_between(epochs, y_mean-y_std, y_mean+y_std, alpha=0.2)\n",
    "ax[1].set_xlabel('$n_{epoch}$')\n",
    "ax[1].set_ylabel('$y=u(0,x)$')\n",
    "ax[1].set_ylim([4.55,4.65])\n",
    "ax[1].set_title('Mean $u(0,x)$')\n",
    "\n",
    "\n",
    "ax[2].semilogy(epochs, L1rel_mean, color=color)\n",
    "ax[2].fill_between(epochs, L1rel_mean-L1rel_std, L1rel_mean+L1rel_std, alpha=0.2,color=color)\n",
    "ax[2].set_xlabel('$n_{epoch}$')\n",
    "ax[2].set_ylabel('$L1_rel$')\n",
    "ax[2].set_ylim([1e-5,5*1e-1])\n",
    "ax[2].set_title('Mean rel. error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYfmf8N-61DG"
   },
   "source": [
    "### Example 2: Allen-Cahn equation\n",
    "Next, we derive a solver for the $d$-dimensional Allen-Cahn equation with \"double-well potential\" which is characterized by the semilinear PDE\n",
    "\n",
    "$$\n",
    "u_t(t,x) + \\Delta u(t,x) + u(t,x) - u^3(t,x) = 0.\n",
    "$$\n",
    "\n",
    "See Section 4.4 in the paper for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAkMhVJp61DG"
   },
   "outputs": [],
   "source": [
    "class AllenCahnSolver(DeepBSDE):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.y_star = .052802\n",
    "\n",
    "        \n",
    "    # Running cost\n",
    "    @tf.function\n",
    "    def fun_f(self, t, x, y, z):\n",
    "        return y - tf.pow(y,3)\n",
    "\n",
    "    # Terminal/Final cost\n",
    "    @tf.function\n",
    "    def fun_g(self, x):\n",
    "        return 1./(2. + 2./5 * tf.reduce_sum(tf.square(x), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter settings for experiments carried out in the paper, Section 4.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fM46gB262Nje"
   },
   "outputs": [],
   "source": [
    "num_exp = 5\n",
    "batch_size = 64\n",
    "lr = 5e-4\n",
    "dim = 100\n",
    "History = []\n",
    "prob_id = 4\n",
    "if prob_id == 1:\n",
    "    # Exp 1: Orig\n",
    "    num_neurons = 110\n",
    "    num_hidden_layers = 2\n",
    "    num_time_steps = 20\n",
    "    suffix = 'orig'\n",
    "elif prob_id == 2:\n",
    "    # Exp 2: 3 layers\n",
    "    num_neurons = 200\n",
    "    num_hidden_layers = 3\n",
    "    num_time_steps = 30\n",
    "    suffix = '3layers'\n",
    "elif prob_id == 3:\n",
    "    # Exp 3: 5 layers\n",
    "    num_neurons = 300\n",
    "    num_hidden_layers = 5\n",
    "    num_time_steps = 50\n",
    "    suffix = '5layers'\n",
    "elif prob_id == 4:\n",
    "    # Exp 4: simple model\n",
    "    num_neurons = 110\n",
    "    num_hidden_layers = 0\n",
    "    num_time_steps = 1\n",
    "    suffix = 'simple'\n",
    "    \n",
    "\n",
    "for j in range(num_exp):\n",
    "    print('{:s}\\nStart of run {:d}\\n{:s}'.format(70*'-',j+1,70*'-'))\n",
    "\n",
    "    # Experiment 2: Allen-Cahn equation\n",
    "    model = AllenCahnSolver(t1=3./10,\n",
    "                            time_steps=num_time_steps,\n",
    "                            dim=dim,\n",
    "                            learning_rate=lr,\n",
    "                            num_neurons=num_neurons,\n",
    "                            num_hidden_layers=num_hidden_layers)\n",
    "    \n",
    "    history = experiment(model, num_epochs=4000)\n",
    "    History.append(history)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNcW4Qzz8toq"
   },
   "source": [
    "Write statistics to `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlX_R5Qu8r6f"
   },
   "outputs": [],
   "source": [
    "X=np.stack(History)\n",
    "epochs = X[0,:,0].astype(int)\n",
    "\n",
    "# Derive statistics for table\n",
    "Loss_mean = np.mean(X[:,:,1], axis=0)\n",
    "Loss_std = np.std(X[:,:,1], axis=0)\n",
    "y_mean = np.mean(X[:,:,2], axis=0)\n",
    "y_std = np.std(X[:,:,2], axis=0)\n",
    "L1rel_mean = np.mean(X[:,:,3], axis=0)\n",
    "L1rel_std = np.std(X[:,:,3], axis=0)\n",
    "L1abs_mean = np.mean(X[:,:,4], axis=0)\n",
    "L1abs_std = np.std(X[:,:,4], axis=0)\n",
    "Time_mean = np.mean(X[:,:,5], axis=0)\n",
    "Time_std = np.std(X[:,:,5], axis=0)\n",
    "\n",
    "# Write table\n",
    "SAVE_CSV = True\n",
    "if SAVE_CSV:\n",
    "    import csv\n",
    "    with open('allen-cahn_dim_{:03d}_{:s}.csv'.format(dim, suffix),'w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['Iter', 'LossMean', 'LossStd', 'yMean', 'yStd', 'L1relMean', 'L1relStd', 'L1absMean', 'L1absStd', 'TimeMean','TimeStd'])\n",
    "        for i in epochs:\n",
    "            csv_out.writerow((i, Loss_mean[i], Loss_std[i],\n",
    "                              y_mean[i], y_std[i], L1rel_mean[i], L1rel_std[i],\n",
    "                              L1abs_mean[i], L1abs_std[i], Time_mean[i], Time_std[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6o7ZuPTV8_x8"
   },
   "source": [
    "Finally, we may plot some loss curves and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16tXZfDV87Ub"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(15,6))\n",
    "color = 'tab:blue'\n",
    "ax[0].semilogy(epochs, Loss_mean, color=color)\n",
    "ax[0].fill_between(epochs, Loss_mean-Loss_std, Loss_mean+Loss_std, alpha=0.2)\n",
    "ax[0].set_xlabel('$n_{epoch}$')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Mean loss')\n",
    "\n",
    "ax[1].plot(epochs, y_mean, color=color)\n",
    "ax[1].fill_between(epochs, y_mean-y_std, y_mean+y_std, alpha=0.2)\n",
    "ax[1].set_xlabel('$n_{epoch}$')\n",
    "ax[1].set_ylabel('$y=u(0,x)$')\n",
    "#ax[1].set_ylim([0.053,0.09])\n",
    "ax[1].set_title('Mean $u(0,x)$')\n",
    "\n",
    "\n",
    "ax[2].semilogy(epochs, L1rel_mean, color=color)\n",
    "ax[2].fill_between(epochs, L1rel_mean-L1rel_std, L1rel_mean+L1rel_std, alpha=0.2,color=color)\n",
    "ax[2].set_xlabel('$n_{epoch}$')\n",
    "ax[2].set_ylabel('$L1_rel$')\n",
    "#ax[2].set_ylim([1e-3,1e-0])\n",
    "ax[2].set_title('Mean rel. error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44IF_6x60cXU"
   },
   "source": [
    "### Example 3: Burgers equation\n",
    "Finally, we construct a solver for the following $d$-dimensional Burgers equation (with coefficients $\\mu \\equiv 0$ and $\\sigma \\equiv d \\, I_{d \\times d}$)\n",
    "\n",
    "$$\n",
    "u_t(t,x) + \\frac{d^2}{2} \\Delta u(t,x) =\n",
    "- \\Big(u(t,x) - \\frac{2+d}{2\\,d}\\Big)\n",
    "\\Big( d \\sum_{i=1}^d \\partial_{x_i} u (t,x) \\Big)\n",
    "$$\n",
    "\n",
    "with final data\n",
    "\n",
    "$$\n",
    "u(1,x) = 1 - \\Big(\n",
    "1+\\exp \\big( 1 + \\frac{1}{d} \\sum_{i=1}^d x_i \\big)\n",
    "    \\Big)^{-1}\n",
    "$$\n",
    "\n",
    "and explicitly known solution\n",
    "\n",
    "$$\n",
    "u(t,x) = 1 - \\Big(\n",
    "1+\\exp \\big( t + \\frac{1}{d} \\sum_{i=1}^d x_i \\big)\n",
    "    \\Big)^{-1},\n",
    "$$\n",
    "\n",
    "see also Section 4.5 and Lemma 4.3 in [E, Han, Jentzen (2017)](https://arxiv.org/abs/1706.04702)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Y-R565m61DD"
   },
   "outputs": [],
   "source": [
    "class BurgersSolver(DeepBSDE):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sigma = self.dim\n",
    "        \n",
    "        z = tf.math.exp( self.t0 + 1./self.dim*tf.reduce_sum(self.x) ).numpy()\n",
    "        self.y_star = 1 - 1./(1.+z) \n",
    "        \n",
    "    # Running cost\n",
    "    @tf.function\n",
    "    def fun_f(self, t, x, y, z):\n",
    "        return (y - (2+self.dim)/(2*self.dim)) * tf.reduce_sum(z, axis=1, keepdims=True)\n",
    "\n",
    "    # Terminal/Final cost\n",
    "    @tf.function\n",
    "    def fun_g(self, x):\n",
    "        z = tf.math.exp( self.t1 + 1./self.dim*tf.reduce_sum(x, axis=1, keepdims=True) )\n",
    "        return 1 - 1./( 1. + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhG-CHJ6acec"
   },
   "outputs": [],
   "source": [
    "# Experiment 3: Burgers equation\n",
    "num_epochs=60000\n",
    "batch_size=64\n",
    "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([30000,50000],[1e-2,1e-3,1e-4])\n",
    "model = BurgersSolver(t1=1.,\n",
    "                      time_steps=80,\n",
    "                      dim=20,\n",
    "                      num_neurons=30,\n",
    "                      learning_rate=lr)\n",
    "\n",
    "# Initialize header\n",
    "print('  Iter        Loss        y   L1_rel    L1_abs   |   Time  Stepsize')\n",
    "\n",
    "# Init timer and history list\n",
    "t0 = time()\n",
    "history=[]\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    inp = model.draw_X_and_dW(batch_size)\n",
    "    loss = model.train(inp)\n",
    "\n",
    "    # Get current Y_0 \\approx u(0,x)\n",
    "    y = model.u0.numpy()[0]\n",
    "\n",
    "    currtime = time() - t0\n",
    "    l1abs = np.abs(y - model.y_star)\n",
    "    l1rel = l1abs / model.y_star\n",
    "\n",
    "    hentry = (i, loss.numpy(), y, l1rel, l1abs, currtime, lr(i))\n",
    "    history.append(hentry)\n",
    "    if i%100 == 0:\n",
    "        print('{:5d} {:12.4f} {:8.4f} {:8.4f}  {:8.4f}   | {:6.1f}  {:6.2e}'.format(*hentry))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QHIyhJEBzTz"
   },
   "source": [
    "Plot statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrdjXs1gBx6d"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(15,6))\n",
    "color = 'tab:blue'\n",
    "X = np.stack(history)\n",
    "epochs = X[:,0]\n",
    "loss = X[:,1]\n",
    "ax[0].semilogy(epochs, loss, color=color)\n",
    "ax[0].set_xlabel('$n_{epoch}$')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Loss')\n",
    "\n",
    "yval = X[:,2]\n",
    "ax[1].plot(epochs, yval, color=color)\n",
    "ax[1].set_xlabel('$n_{epoch}$')\n",
    "ax[1].set_ylabel('$y=u(0,x)$')\n",
    "ax[1].set_title('Value $u(0,x)$')\n",
    "\n",
    "l1rel = X[:,3]\n",
    "ax[2].semilogy(epochs, l1rel, color=color)\n",
    "ax[2].set_xlabel('$n_{epoch}$')\n",
    "ax[2].set_ylabel('$L1_rel$')\n",
    "#ax[2].set_ylim([1e-5,5*1e-1])\n",
    "ax[2].set_title('Rel. error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepBSDE_Solver.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
